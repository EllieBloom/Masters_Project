{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc65ac0a-7dd3-43f8-9003-de5a633456d9",
   "metadata": {},
   "source": [
    "# Exploring time series in python\n",
    "\n",
    "**Date started**: 21st June 2022\n",
    "\n",
    "**Guide**: https://rishi-a.github.io/2020/05/25/granger-causality.html\n",
    "\n",
    "**Factors to consider altering/adding**: \n",
    "* Normalisation\n",
    "* Transformation (log) - *see script 2*\n",
    "* Checking all VAR fit stats\n",
    "* Lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7256c70f-3132-4e27-84ee-d55b8c40f968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elliebloom/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/Users/elliebloom/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n"
     ]
    }
   ],
   "source": [
    "# Packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import statsmodels.tsa.stattools as smt\n",
    "from statsmodels.tsa.api import VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005444be-2127-4451-a393-70ee13275ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to install any packages required in general:\n",
    "\n",
    "# import subprocess\n",
    "# import sys\n",
    "\n",
    "# def install(package):\n",
    "#    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    \n",
    "# install(\"johansen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b500981-841c-45b8-926f-1bf517a89ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datetime.datetime'>\n",
      "Start date is: 2020-05-01 00:00:00, i.e. start of REACT data\n",
      "End date is: 2020-10-05 00:00:00, i.e. one month before the end of lockdown 1\n"
     ]
    }
   ],
   "source": [
    "# Useful dates\n",
    "\n",
    "\n",
    "bank_holidays = [\"2020-01-01\",\"2020-04-10\",\"2020-04-13\",\"2020-05-08\",\n",
    "                \"2020-05-25\",\"2020-08-31\",\"2020-12-25\",\"2020-12-28\",\n",
    "                \"2021-01-01\",\"2021-04-02\",\"2021-04-05\",\"2021-05-03\",\n",
    "                \"2021-05-31\",\"2021-08-30\",\"2021-12-27\",\"2021-12-28\",\n",
    "                \"2022-01-03\",\"2022-04-15\",\"2022-04-18\",\"2022-05-02\"]\n",
    "\n",
    "\n",
    "type(bank_holidays[0]) # They are currently strings\n",
    "bank_holidays = [dt.datetime.strptime(item, \"%Y-%m-%d\") for item in bank_holidays]\n",
    "type(bank_holidays[0]) # Now datetime objects\n",
    "\n",
    "lockdown_1_start = dt.datetime.strptime(\"2020-03-26\",\"%Y-%m-%d\")\n",
    "lockdown_1_end = dt.datetime.strptime(\"2020-06-15\",\"%Y-%m-%d\")\n",
    "\n",
    "lockdown_2_start = dt.datetime.strptime(\"2020-11-05\",\"%Y-%m-%d\")\n",
    "lockdown_2_end = dt.datetime.strptime(\"2020-12-02\",\"%Y-%m-%d\")\n",
    "\n",
    "lockdown_3_start = dt.datetime.strptime(\"2021-01-06\",\"%Y-%m-%d\")\n",
    "lockdown_3_end = dt.datetime.strptime(\"2021-04-21\",\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "REACT_start = dt.datetime.strptime(\"2020-05-01\",\"%Y-%m-%d\")\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "start_date = REACT_start\n",
    "print(type(start_date))\n",
    "end_date = lockdown_2_start-relativedelta(months=1)\n",
    "\n",
    "print(f'Start date is: {start_date}, i.e. start of REACT data')\n",
    "print(f'End date is: {end_date}, i.e. one month before the end of lockdown 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9455d4a1-c8cf-4cef-ac34-4e1de75b2062",
   "metadata": {},
   "source": [
    "## Time Series\n",
    "\n",
    "Exploring Granger Causality using tutorial - https://rishi-a.github.io/2020/05/25/granger-causality.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "837c303a-8121-4390-b22d-04363ca73811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "\n",
    "mob_long = pd.read_csv(\"/Users/elliebloom/Desktop/Masters/Project/Analysis/Mapping/Outputs/google_regional_long.csv\", index_col='date',parse_dates=['date'])\n",
    "mob_wide = pd.read_csv(\"/Users/elliebloom/Desktop/Masters/Project/Analysis/Mapping/Outputs/google_regional_wide.csv\", index_col='date',parse_dates=['date'])\n",
    "prev = pd.read_csv(\"/Users/elliebloom/Desktop/Masters/Project/Analysis/Time_series_analysis/Ouputs/Tibbles/prev_smooth_tibble.csv\", index_col='d_comb',parse_dates=['d_comb'])\n",
    "\n",
    "mob_long.index = pd.to_datetime(mob_long.index)\n",
    "mob_wide.index = pd.to_datetime(mob_wide.index)\n",
    "mob_wide = mob_wide.sort_index()\n",
    "prev = prev.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b931bce9-8569-46b0-930a-2f360ed131f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the long dataset are:\n",
      "Unnamed: 0\n",
      "region\n",
      "type_mobility\n",
      "mobility\n",
      "Column names in the wide dataset are:\n",
      "Unnamed: 0\n",
      "region\n",
      "retail_recreation\n",
      "grocery_pharmacy\n",
      "parks\n",
      "transit_stations\n",
      "workplaces\n",
      "residential\n",
      "retail_recreation_av\n",
      "grocery_pharmacy_av\n",
      "parks_av\n",
      "transit_stations_av\n",
      "workplaces_av\n",
      "residential_av\n",
      "Column names in the prevalence dataset are:\n",
      "Unnamed: 0\n",
      "x\n",
      "p\n",
      "lb_2.5\n",
      "lb_25\n",
      "ub_97.5\n",
      "ub_75\n",
      "region\n"
     ]
    }
   ],
   "source": [
    "print(\"Column names in the long dataset are:\")\n",
    "for col in mob_long.columns:\n",
    "    print(col)\n",
    "    \n",
    "print(\"Column names in the wide dataset are:\")\n",
    "for col in mob_wide.columns:\n",
    "    print(col)\n",
    "    \n",
    "print(\"Column names in the prevalence dataset are:\")\n",
    "for col in prev.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fe9aac7-528a-4189-bfeb-00d590215973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>region</th>\n",
       "      <th>retail_recreation</th>\n",
       "      <th>grocery_pharmacy</th>\n",
       "      <th>parks</th>\n",
       "      <th>transit_stations</th>\n",
       "      <th>workplaces</th>\n",
       "      <th>residential</th>\n",
       "      <th>retail_recreation_av</th>\n",
       "      <th>grocery_pharmacy_av</th>\n",
       "      <th>parks_av</th>\n",
       "      <th>transit_stations_av</th>\n",
       "      <th>workplaces_av</th>\n",
       "      <th>residential_av</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-02-15</th>\n",
       "      <td>new.2662</td>\n",
       "      <td>NORTH EAST</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>-4.625000</td>\n",
       "      <td>-33.875000</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>-1.875000</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>-31.553571</td>\n",
       "      <td>-12.357143</td>\n",
       "      <td>-5.636905</td>\n",
       "      <td>-25.553571</td>\n",
       "      <td>-27.910714</td>\n",
       "      <td>9.681973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-15</th>\n",
       "      <td>new.5922</td>\n",
       "      <td>WEST MIDLANDS</td>\n",
       "      <td>-12.571429</td>\n",
       "      <td>-6.428571</td>\n",
       "      <td>-41.000000</td>\n",
       "      <td>-11.285714</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>-34.408163</td>\n",
       "      <td>-13.591837</td>\n",
       "      <td>-4.268707</td>\n",
       "      <td>-29.755102</td>\n",
       "      <td>-30.102041</td>\n",
       "      <td>10.452381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-15</th>\n",
       "      <td>new.1032</td>\n",
       "      <td>EAST MIDLANDS</td>\n",
       "      <td>-9.222222</td>\n",
       "      <td>-5.222222</td>\n",
       "      <td>-35.125000</td>\n",
       "      <td>-7.375000</td>\n",
       "      <td>-4.333333</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>-34.214286</td>\n",
       "      <td>-13.607143</td>\n",
       "      <td>-3.178571</td>\n",
       "      <td>-31.607143</td>\n",
       "      <td>-29.884921</td>\n",
       "      <td>10.265873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-15</th>\n",
       "      <td>new.3477</td>\n",
       "      <td>NORTH WEST</td>\n",
       "      <td>-9.200000</td>\n",
       "      <td>-5.700000</td>\n",
       "      <td>-34.555556</td>\n",
       "      <td>-11.200000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>-31.414286</td>\n",
       "      <td>-12.671429</td>\n",
       "      <td>2.460317</td>\n",
       "      <td>-29.242857</td>\n",
       "      <td>-29.628571</td>\n",
       "      <td>10.186111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-15</th>\n",
       "      <td>new.1847</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>-11.000000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>-34.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-39.428571</td>\n",
       "      <td>-17.714286</td>\n",
       "      <td>-10.714286</td>\n",
       "      <td>-41.285714</td>\n",
       "      <td>-36.714286</td>\n",
       "      <td>13.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Unnamed: 0         region  retail_recreation  grocery_pharmacy  \\\n",
       "date                                                                        \n",
       "2020-02-15   new.2662     NORTH EAST          -6.375000         -4.625000   \n",
       "2020-02-15   new.5922  WEST MIDLANDS         -12.571429         -6.428571   \n",
       "2020-02-15   new.1032  EAST MIDLANDS          -9.222222         -5.222222   \n",
       "2020-02-15   new.3477     NORTH WEST          -9.200000         -5.700000   \n",
       "2020-02-15   new.1847         LONDON         -11.000000         -8.000000   \n",
       "\n",
       "                parks  transit_stations  workplaces  residential  \\\n",
       "date                                                               \n",
       "2020-02-15 -33.875000         -7.000000   -1.875000     1.833333   \n",
       "2020-02-15 -41.000000        -11.285714   -4.000000     2.285714   \n",
       "2020-02-15 -35.125000         -7.375000   -4.333333     1.875000   \n",
       "2020-02-15 -34.555556        -11.200000   -3.000000     2.111111   \n",
       "2020-02-15 -34.000000        -10.000000   -3.000000     2.000000   \n",
       "\n",
       "            retail_recreation_av  grocery_pharmacy_av   parks_av  \\\n",
       "date                                                               \n",
       "2020-02-15            -31.553571           -12.357143  -5.636905   \n",
       "2020-02-15            -34.408163           -13.591837  -4.268707   \n",
       "2020-02-15            -34.214286           -13.607143  -3.178571   \n",
       "2020-02-15            -31.414286           -12.671429   2.460317   \n",
       "2020-02-15            -39.428571           -17.714286 -10.714286   \n",
       "\n",
       "            transit_stations_av  workplaces_av  residential_av  \n",
       "date                                                            \n",
       "2020-02-15           -25.553571     -27.910714        9.681973  \n",
       "2020-02-15           -29.755102     -30.102041       10.452381  \n",
       "2020-02-15           -31.607143     -29.884921       10.265873  \n",
       "2020-02-15           -29.242857     -29.628571       10.186111  \n",
       "2020-02-15           -41.285714     -36.714286       13.714286  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mob_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51f4dbf9-911d-41c8-a3fa-c46c27d3fb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focusing on London\n",
    "mob_london = mob_wide[mob_wide['region']==\"LONDON\"]\n",
    "prev_london = prev[prev['region']==\"LONDON\"]\n",
    "\n",
    "# Only including columns of interest\n",
    "mob_london = mob_london[['workplaces_av']]\n",
    "prev_london = prev_london[['p']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbc06e75-6313-4cf4-9393-3f8233a2c6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag used is -42 days\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workplaces_av</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-02-15</th>\n",
       "      <td>-35.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-16</th>\n",
       "      <td>-43.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-17</th>\n",
       "      <td>-49.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-18</th>\n",
       "      <td>-58.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-19</th>\n",
       "      <td>-57.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            workplaces_av\n",
       "date                     \n",
       "2020-02-15     -35.142857\n",
       "2020-02-16     -43.571429\n",
       "2020-02-17     -49.714286\n",
       "2020-02-18     -58.714286\n",
       "2020-02-19     -57.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_lag = -6*7\n",
    "print(f'Lag used is {shift_lag} days')\n",
    "\n",
    "mob_london['workplaces_av']=mob_london['workplaces_av'].shift(shift_lag)\n",
    "mob_london.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3eb429f-1ff6-4bc6-8941-570aff1acfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focussing on specified interval\n",
    "\n",
    "mob_london = mob_london.loc[start_date:end_date]\n",
    "\n",
    "prev_london = prev_london.loc[start_date:end_date]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e14288-248c-491f-a3d8-76807b84d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging into a combined London dataframe\n",
    "\n",
    "london = pd.merge(mob_london, prev_london, how='inner', left_index=True, right_index=True)\n",
    "london.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf21915-5174-49f0-a9b8-d4a7851050a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the columns that aren't needed\n",
    "\n",
    "london = london[['workplaces_av','p']]\n",
    "london"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1e5b2-8162-4313-bcec-73ab942f2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "   \n",
    "f2, (ax4, ax5) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "f2.tight_layout()\n",
    "\n",
    "lag_plot(london['workplaces_av'], ax=ax4)\n",
    "ax4.set_title('workplaces rolling av');\n",
    "\n",
    "lag_plot(london['p'], ax=ax5)\n",
    "ax5.set_title('prevalence');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba984ca-7c70-47c1-8226-0708b804834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_london = london.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb86335-6bef-4b5e-a9ed-9924b9dcfefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differencing - using the rolling average as clear differencing needed, and as well as prevalence\n",
    "\n",
    "london['workplaces_av'] = london['workplaces_av'] - london['workplaces_av'].shift(1)\n",
    "london['p'] = london['p'] - london['p'].shift(1)\n",
    "london = london.dropna()\n",
    "london.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ffcbb2-bf41-4dd3-b46f-dd61e14b06d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "   \n",
    "f2, (ax4, ax5) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "f2.tight_layout()\n",
    "\n",
    "lag_plot(london['workplaces_av'], ax=ax4)\n",
    "ax4.set_title('workplaces rolling av - d1');\n",
    "\n",
    "lag_plot(london['p'], ax=ax5)\n",
    "ax5.set_title('prevalence -d1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232ee4b1-ecd7-46d1-ac42-a7910ce2ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADF Null hypothesis: there is a unit root, meaning series is non-stationary\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "X1 = np.array(london['workplaces_av'])\n",
    "X1 = X1[~np.isnan(X1)]\n",
    "\n",
    "result = adfuller(X1)\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "    \n",
    "    \n",
    "X2 = np.array(london['p'])\n",
    "X2 = X2[~np.isnan(X2)]\n",
    "\n",
    "result = adfuller(X2)\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef7c86e-dd7f-4724-8df7-ba61024f4156",
   "metadata": {},
   "outputs": [],
   "source": [
    "## KPSS Null hypothesis: there is a no unit root, meaning series is stationary\n",
    "\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "def kpss_test(series, **kw):    \n",
    "    statistic, p_value, n_lags, critical_values = kpss(series, **kw)\n",
    "    # Format Output\n",
    "    print(f'KPSS Statistic: {statistic}')\n",
    "    print(f'p-value: {p_value}')\n",
    "    print(f'num lags: {n_lags}')\n",
    "    print('Critial Values:')\n",
    "    for key, value in critical_values.items():\n",
    "        print(f'   {key} : {value}')\n",
    "    print(f'Result: The series is {\"not \" if p_value < 0.05 else \"\"}stationary')\n",
    "\n",
    "kpss_test(X1)\n",
    "kpss_test(X2)\n",
    "\n",
    "# now stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3519bf-756e-40b9-97cd-db8f09b0b015",
   "metadata": {},
   "source": [
    "Overall result to far: the data is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf003b-c4bd-429d-985c-f10e7ee832e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test. We will need this later for VAR analysis\n",
    "\n",
    "msk = np.random.rand(len(london)) < 0.8\n",
    "train = london[msk]\n",
    "test = london[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d0b9fb-f4f6-4ea6-bad8-78cc877d2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "   \n",
    "f2, (ax4, ax5) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "f2.tight_layout()\n",
    "\n",
    "lag_plot(london['workplaces_av'], ax=ax4)\n",
    "ax4.set_title('workplaces_av - d1');\n",
    "\n",
    "lag_plot(london['p'], ax=ax5)\n",
    "ax5.set_title('p - d1');\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f317cb-2fcd-4177-856f-bf74f5eaf806",
   "metadata": {},
   "source": [
    "Result: lag plot is confirmatory with ADF test and KPSS. Uses moving average and 3 differences\n",
    "\n",
    "Next: setting up VAR model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9fb408-2290-4ae3-b147-cc1b53a81bbc",
   "metadata": {},
   "source": [
    "**Step 2**: Let the maximum order of integration for the group of time-series be d. So, if there are two time-series and one is found to be I(1) and the other is I(2), then d = 2. If one is I(0) and the other is I(1), then m = 1, etc.\n",
    "\n",
    "**Result**: Saw above that in this case d=1\n",
    "\n",
    "**Step 3**: Set up a VAR model in the levels of the data, regardless of the orders of integration of the various time-series. Most importantly, **you must not difference the data, no matter what you found at Step 1**.\n",
    "\n",
    "**Step 4**: Determine the appropriate maximum lag length for the variables in the VAR, say p, using the usual methods. Specifically, base the choice of p on the usual information criteria, such as AIC, SIC.\n",
    "\n",
    "\n",
    "**Note**: try running like this first, then incorporate info on the lag later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c2057-da6b-4d28-ab82-03b9bd88b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the lag P for VAR\n",
    "\n",
    "raw_london = raw_london.dropna()\n",
    "model = VAR(np.array(raw_london)) #recall that rawData is w/o difference operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920adb38-4799-42cf-bf49-abaee287b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,2,3,4,5,6,7,8,9,10,11,12]:\n",
    "    result = model.fit(i)\n",
    "    try:\n",
    "        print('Lag Order =', i)\n",
    "        print('AIC : ', result.aic)\n",
    "        print('BIC : ', result.bic)\n",
    "        print('FPE : ', result.fpe)\n",
    "        print('HQIC: ', result.hqic, '\\n')\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25fca86-49a1-4c13-a3e8-8bd1aeadb860",
   "metadata": {},
   "source": [
    "**Result**: The lag order p=6 on the basis of AIC  \n",
    "**Step 5**: Make sure that the VAR is well-specified. For example, ensure that there is no serial correlation in the residuals. If need be, increase p until any autocorrelation issues are resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda8c9e3-5e73-4a23-858e-cecbab78efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAR(np.asarray(train))\n",
    "model_fitted = model.fit(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a507c5-2d61-4ea3-9726-4e70b474f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for Serial Correlation of Residuals (Errors) using Durbin Watson Statistic\n",
    "#The value of this statistic can vary between 0 and 4. \n",
    "#The closer it is to the value 2, then there is no significant serial correlation. \n",
    "#The closer to 0, there is a positive serial correlation, \n",
    "#and the closer it is to 4 implies negative serial correlation.\n",
    "\n",
    "\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "out = durbin_watson(model_fitted.resid)\n",
    "\n",
    "for col, val in zip(london.columns, out):\n",
    "    print(col, ':', round(val, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa550a6-fc0e-4ed7-896c-7a5c19055f20",
   "metadata": {},
   "source": [
    "**Result**: There is no significant correlation between in the residuals\n",
    "\n",
    "**Step 6**: If two or more of the time-series have the same order of integration, at Step 1, then test to see if they are cointegrated, preferably using Johansen’s methodology (based on your VAR) for a reliable result.  \n",
    "\n",
    "**Johansen's test**:   \n",
    "H0: There are no cointegrating equations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260c166-c794-4333-a5d2-96300a8194d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.tsa.vector_ar.vecm import JohansenTestResult\n",
    "\n",
    "# Function from here : http://web.pdx.edu/~crkl/ceR/Python/example14_3.py\n",
    "\n",
    "\"\"\"\n",
    "    Johansen cointegration test of the cointegration rank of a VECM\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    endog : array_like (nobs_tot x neqs)\n",
    "        Data to test\n",
    "    det_order : int\n",
    "        * -1 - no deterministic terms - model1\n",
    "        * 0 - constant term - model3\n",
    "        * 1 - linear trend\n",
    "    k_ar_diff : int, nonnegative\n",
    "        Number of lagged differences in the model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def joh_output(res):\n",
    "    output = pd.DataFrame([res.lr2,res.lr1],\n",
    "                          index=['max_eig_stat',\"trace_stat\"])\n",
    "    print(output.T,'\\n')\n",
    "    print(\"Critical values(90%, 95%, 99%) of max_eig_stat\\n\",res.cvm,'\\n')\n",
    "    print(\"Critical values(90%, 95%, 99%) of trace_stat\\n\",res.cvt,'\\n')\n",
    "\n",
    "result = coint_johansen(np.asarray(london),0,1)\n",
    "joh_output(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9134484d-651b-4bdd-82a1-66a53674c885",
   "metadata": {},
   "source": [
    "**Result**: Greater than all critical values, thus p<0.05 - Thus rejecting the null hypothesis. Thus they are cointegrated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e08cf-7525-4fc6-b8c0-076fd5d270d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying the other test done in the tutorial\n",
    "# This uses the augmented Engle-Granger two-step cointegration test. Constant or trend is included in 1st stage regression, i.e. in cointegrating equation.\n",
    "\n",
    "import statsmodels.tsa.stattools as ts \n",
    "\n",
    "result=ts.coint(london['workplaces_av'], london['p'], return_results=True)\n",
    "# Note the results includes: t stat, p value, [1% critical value, 5% critical value, 10% critical value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136f48d3-47c3-4fa5-92ee-fb83ba46c8ca",
   "metadata": {},
   "source": [
    "**Result** : The p-valus < 0.05. Thus rejecting the null hypothesis. Thus they are cointegrated using this alternative test too.\n",
    "\n",
    "\n",
    "**Step 7**: No matter what you conclude about cointegration at Step 6, this is not going to affect what follows. It just provides a possible cross-check on the validity of your results at the very end of the analysis.  \n",
    "**Step 8**: Now take the preferred VAR model and add in m additional lags of each of the variables into each of the equations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c49e2ed-16b2-4cff-8381-72794bb3ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAR(np.asarray(train))\n",
    "model_fitted = model.fit(6)\n",
    "#get the lag order\n",
    "lag_order = model_fitted.k_ar\n",
    "print(lag_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca7214a-d600-402e-98f1-1491350cfa99",
   "metadata": {},
   "source": [
    "**Step 9 and Step 10**: Test for Granger non-causality. It’s essential that you don’t include the coefficients for the ‘extra’ m lags when you perform the tests. They are there just to fix up the asymptotics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad608f63-2c22-4f73-aa30-8d9364508b4e",
   "metadata": {},
   "source": [
    "^ I'm not sure I've done step 8 right, and not sure what it means by the 'extra' lags above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb73c7b-3974-4b50-a864-e047b8ce3457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "maxlag=lag_order #becuase we got this value before. We are not suppose to add 1 to it\n",
    "test = 'ssr_chi2test'\n",
    "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n",
    "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
    "    The rows are the response variable, columns are predictors. The values in the table \n",
    "    are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
    "    the Null Hypothesis that the coefficients of the corresponding past values is \n",
    "    zero, that is, the X does not cause Y can be rejected.\n",
    "\n",
    "    data      : pandas dataframe containing the time series variables\n",
    "    variables : list containing names of the time series variables.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df\n",
    "\n",
    "o = grangers_causation_matrix(train, variables = train.columns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99400c4-0ce1-4b0c-b577-428f52f84094",
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a91879-1f68-4dfa-9453-77a34aafb148",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"p value for prevalence causing mobility is:\",o.iloc[0,1],'\\n')\n",
    "print(\"p value for mobility causing prevalence is:\",o.iloc[1,0],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6d06d-fed8-450d-9f33-e84cc2ce09fc",
   "metadata": {},
   "source": [
    "**Result**: If a given p-value is < significance level (0.05), then, the corresponding X series (column) causes the Y (row). // Both p values < 0.05 therefore they each cause eachother...\n",
    "\n",
    "**Step 11**: Finally, look back at what you concluded in Step 6 about cointegration.  \n",
    "\n",
    "If two or more time-series are cointegrated, then there must be Granger causality between them - either one-way or in both directions. However, the converse is not true.\n",
    "\n",
    "So, if your data are cointegrated but you don't find any evidence of causality, you have a conflict in your results. (This might occur if your sample size is too small to satisfy the asymptotics that the cointegration and causality tests rely on.) If you have cointegration and find one-way causality, everything is fine. (You may still be wrong about there being no causality in the other direction.) If your data are not cointegrated, then you have no cross-check on your causality results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df97c8c-b878-4a8c-86a8-ce06f94c337c",
   "metadata": {},
   "source": [
    "**Now diverging from the tutorial, exploring forecasting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb886e5-6db7-4997-976f-fb4e6d0dd971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring forecasting\n",
    "\n",
    "# Firstly summary of the model\n",
    "model\n",
    "model_fitted = model.fit(6)\n",
    "model_fitted.summary()\n",
    "# y1 is presumably -> mobility, y2 is presumably -> prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40e1aed-fd56-4db1-bd7c-f24af0bbda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Despite confirming that mobility Granger causes prevalence, none of the coefficients are significant for mobility (y1) for the equation for y2 (prevalence). However, nor are any of the prevalence ones either??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bdb43c-92b9-481d-9e35-43f14a9a5900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2021/08/vector-autoregressive-model-in-python/\n",
    "\n",
    "lagged_Values = train.values[-8:]\n",
    "pred = model_fitted.forecast(y=lagged_Values, steps=12) \n",
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
